\section{Introduction}
\label{sec:introduction}

% why do measurement
The measurement of Internet has become increasingly important and challenging due to its complexity, large-scale, heterogeneity and variability. 

% why do latency

% split up to three parts
Our work has three distinct part: the latency of WAN, the latency experienced by end-user when doing DC related service, and the latency within cellular networks.

The first measurement tries to understand how the latency of the WAN is related to the geo-location. The traditional model used to calculate the latency between end-hosts is to sum propagation delay, transmission delay, queuing delay and processing delay. The propogation delay is is determined the by the length of cables (if wired network), and will have the lower bound, which we will refer to speed-of-light limit in this paper. The transmission delay and processing delay depend on the speed of router/switch and the size of the packet, and they are generally getting smaller and smaller. Queuing delay has a lot of variation depending on the traffic load, and in worst case, it even leads to packet drop. So the question we were asking is, given the recent development of hardware and software, what is the state-of-art latency performance in comparison with the speed-of-light limit? And if there is a huge gap in between, which constitute the major portion? 


% summarize the contribution

% rest of the paper

A numbers of tools have been proposed to accomplish this task (see Sec.\,\ref{sec:related-work} for a complete review). In this project, our primary goal is not to introduce a novel network measurement tool; instead, we adapt existing measurement techniques to characterize the performance of current Internet. We base our work on previous investigations, and try to elucidate the changes in measurement strategies and outcomes. Specifically, the cloud computing service model and mobile computing have re-shaped the Internet in unanticipated ways. The impact of these new models on end-to-end latency and measurement techniques have never been formally studied. We propose the questions listed bellow as motivations for our study.

\begin{itemize}
\item How does network latency vary among different kinds of end hosts -- generic end users(including mobile/wireless end hosts), regular web servers, and cloud service from data centers?
\item How current Internet latency performs in comparison to the speed-of-light limit?
\item How wide-area networks and data centers contribute to end-to-end latency experienced by end users?
\item How latency is affected by edge caching of content, especially on mobile networks?
\item What are the characteristics of time-series analysis for latency on a daily/weekly/yearly basis?
\end{itemize}

Our initial approach will consist of re-implementing and re-producing previous measurement works in the context of current Internet, following the guidelines outlined in \cite{paxson2004strategies}. One major strategy we intend to re-implement is King \cite{gummadi2002king}, a technique which leverages the DNS infrastructure to measure end-to-end latency of arbitrary end hosts. To guarantee deliverables, we have a outlined an initial timetable for our study below (see Table.\,\ref{tab:plan}).

\begin{table}
  \centering
  \begin{tabular}{ c|p{4cm} }
    \hline
    Period & Task \\
    \hline
    Late Feburary & Literature review \\
    March & Re-implementation and initial measurements \\
    April & Revision/correction of any possible methodology flaws \\
    Early May & Wrap up and report \\ 
    \hline
  \end{tabular}
  \label{tab:plan}
  \caption{timeline for project}
\end{table}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
